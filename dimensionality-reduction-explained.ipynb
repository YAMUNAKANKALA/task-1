{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "b9015933-c316-baf7-ae91-8bfafcb77380",
    "_uuid": "a860654779508c43fa22be5392b2d89c5981a574"
   },
   "source": [
    "### Dimensionality Reduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "fe310775-1caa-06b8-10a4-e491708bd883",
    "_uuid": "a9199b90e2095277381837b778adcc1554e4b33a"
   },
   "source": [
    "Inspired by the course of Apostolos N. Papadopoulos ‚Äì Christos Giatsidis \n",
    "and Erwan Lepennec (Polytechnique)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "22946207-b60b-097d-9c02-fa704053fa26",
    "_uuid": "9cad6bab9d78643fdfcdd9b6cff243b8b9bb04d2",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy as sp\n",
    "import scipy.linalg as linalg\n",
    "import sklearn.neighbors as nb\n",
    "import sklearn.utils.graph as ug\n",
    "from time import time\n",
    "from numpy import *\n",
    "from sklearn.datasets import load_iris\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from matplotlib.ticker import NullFormatter\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "from sklearn import manifold, datasets\n",
    "from sklearn.decomposition import PCA\n",
    "from scipy.spatial import distance as spd\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "afb1530d-9b49-7d1b-0b47-1c48516740ed",
    "_uuid": "c4115d03eeb03d2985f0bf55befe34594ebe8ef2"
   },
   "source": [
    "As soon as the dimension is larger than 3, it becomes hard to visualize the raw data. Dimension reduction technique can be used to alleviate this issue by *projecting* the data in a low dimensional space, typically a 2D space. Note that those dimension reduction ideas can also be used a preprocessing step for any learning task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "256e9519-e255-afb2-c9aa-3cf947a2ed41",
    "_uuid": "5332bcdc167ec03065506c3ef7e134c9dc752aba"
   },
   "source": [
    "### recall on SVD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "d09eb5f8-f9b9-4200-25aa-857966e31c9b",
    "_uuid": "d13744aead59b4deee698eae6ff75b3aa217f629"
   },
   "source": [
    "SVD decomposition of $\\textbf{X}_{(n)}$ is $\\textbf{X}_{(n)} = U D(\\lambda) V^t$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "cbf1ce87-d8a1-a6c5-91ba-97b367da8da1",
    "_uuid": "d481c49656ce6355cd7822fc3ef60f9e2b311569"
   },
   "source": [
    "### Own PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "79eca3ac-52d3-7f5f-d5ed-e6eac7568848",
    "_uuid": "962e12b2310673fb60106482120d6ef907211ed6"
   },
   "source": [
    "We will start by the most classical dimension reduction algorithm, the Principal Component Analysis one. The idea is to find a subspace spanned by $d'$ orthonormal columns $V^{(l)}$ such that the reconstruction error between the data and its projection on this subspace\n",
    "\n",
    "\\begin{equation}\n",
    "\\sum_{i=1}^n \\| \\textbf{X}_i - m + V^t (\\textbf{X}_i -m) V\\|^2\n",
    "\\end{equation}\n",
    "\n",
    " is as small as possible. An explicit solution to this problem is given by the space spanned by the $d'$ eigenvectors of the $d \\times d$ empirical covariance matrix of the observations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "699cbc0a-d967-7ca3-0eb4-dfa8d1ec025d",
    "_uuid": "a9a8b2130585c6e01ecc1bb4b0addc7af88bc804"
   },
   "source": [
    "#### Algorithm (main Idea)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "25518829-d4f9-76c7-0610-153ce067f094",
    "_uuid": "f099f754cb7c8ed8c2a391a9138ac3df768f71dc"
   },
   "source": [
    "- Standardize the data.\n",
    "- Obtain the Eigenvectors and Eigenvalues from the covariance matrix or correlation matrix, or perform Singular Vector Decomposition.\n",
    "- Sort eigenvalues in descending order and choose the k eigenvectors that correspond to the k largest eigenvalues where k is the number of dimensions of the new feature subspace (k‚â§d)/.\n",
    "- Construct the projection matrix W from the selected k eigenvectors.\n",
    "- Transform the original dataset X via W to obtain a k-dimensional feature subspace Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "86cbffbb-254f-370d-2203-0ec169325a76",
    "_uuid": "459255d064ee56de3d79eeaade206d461682cfce",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#data : the data matrix\n",
    "#k the number of component to return\n",
    "#return the new data and the variance that was maintained \n",
    "def pca1(data,k):\n",
    "\t# Performs principal components analysis (PCA) on the n-by-p data matrix A (data)\n",
    "\t# Rows of A correspond to observations (wines), columns to variables.\n",
    "\t## TODO: Implement PCA\n",
    "\n",
    "\t# compute the mean\n",
    "\n",
    "    m = np.mean(data,0)\n",
    "\t# subtract the mean (along columns)\n",
    "    C = data-m\n",
    "\t# compute covariance matrix\n",
    "    cov_mat = np.dot(transpose(C),C)\n",
    "\t# compute eigenvalues and eigenvectors of covariance matrix\n",
    "    eigval,eigvect = linalg.eig(cov_mat)\n",
    "\t# Sort eigenvalues (their indexes)\n",
    "    idx = eigval.argsort()[::-1]\n",
    "    eigval[idx]\n",
    "\t# Sort eigenvectors according to eigenvalues\n",
    "    vect = eigvect[:,idx]\n",
    "    eigk = vect[:,0:k]\n",
    "\t# Project the data to the new space (k-D) and measure how much variance we kept\n",
    "    data2 = np.dot(C,eigk)\n",
    "    perc = sum(eigval[0:k])/sum(eigval)\n",
    "    return  (data2, perc)#put here a tuple to return both "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "14b0fc23-5190-03e9-fa2b-496e7ff21b6e",
    "_uuid": "f0ac327e8d643442f9f9d46e649afa4d84b18911"
   },
   "source": [
    "### Own MDS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "b3ec79bc-2d52-a586-ad06-37ff2c78162b",
    "_uuid": "d42355f28e77d9c53843a02b1bef4f7bffbe6f84"
   },
   "source": [
    "Maintain the distances / similarities / dissimilarities among\n",
    "\n",
    "all pairs of elements"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "bc9af35c-f1b5-ffcf-e7a4-e9a74151e86f",
    "_uuid": "e4519c820fb6df0659999df8f888dcd7965793b1"
   },
   "source": [
    "* Let us consider the matrix A of distances $D = A^2$\n",
    "* $J = I_{N} - (11^T)/N$\n",
    "* where $I_{N}$the NxN identity matrix (one‚Äôs in diagonal and zeros elsewhere) and ($11^T$) is the NxN matrix with one‚Äôs at each cell\n",
    "* $B = -1/2JDJ$\n",
    "* $UWV^T$ = svd($B$)\n",
    "* $A‚Ä≤ = U_{k}W1/2_k$ (k largest eigen values and corresponding vectors from U)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "7ca64720-b614-4c08-b27e-aeabe8a708a6",
    "_uuid": "9242c99cd940dba7d61865d965a5c64e5ee02a8a"
   },
   "source": [
    "The classic MDS algorithm works the similarity of\n",
    "the data\n",
    "\n",
    "* PCA works on the similarity of the features\n",
    "* PCA and classic MDS have the same results\n",
    "\n",
    "if Euclidean distance is used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "9f5c46a7-ac53-a335-bf56-ec014efe44bf",
    "_uuid": "6c3a1cd4a91d7969514d4c72706cea716bbff170",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#D :distance matrix\n",
    "#k: number of vectors to use\n",
    "def mds(D,k):\n",
    "    nelem = D.shape[0]\n",
    "    J = eye(nelem) - (1.0/nelem) * ones(nelem)\n",
    "    # Compute matrix B\n",
    "    B = -(1.0/2) * dot(J,dot(pow(D,2),J))\n",
    "    # SVD decomposition of B  \n",
    "    U,L,V = linalg.svd(B)\n",
    "    return dot(U[:,:k],sqrt(diag(L)[:k,:k]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "21190241-484a-261e-4467-295de351d07b",
    "_uuid": "b465aba30ed9ab720afb4f9394c28146ea42cc09"
   },
   "source": [
    "### Own Isomap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "53a6d7c1-8289-07e2-739c-69cce46a3996",
    "_uuid": "594aad34cf8f191b7b656e8d3064da4fb3b6f212"
   },
   "source": [
    "1. Find the k-nearest neighbors of each point\n",
    "2. Construct a graph where each node is connected only to the k-nearest\n",
    "neighbours. Graph Matrix ùê∑ùëî\n",
    "The links are typically weighted with the Euclidian distance\n",
    "3. Find graph distances between all points in the graph. Shortest paths : ùë´ùíà\n",
    "4. Apply MDS on ùë´ùíà\n",
    "(or PCA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "8c54a17f-38a9-dbbf-bc0b-067ca46bff12",
    "_uuid": "42903e214379d95fa6a944c08f37d331f8eaabaf",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def isomap(D,k,n_neighbors):\n",
    "    #k nearest neighbour algorithm\n",
    "    knr = nb.NearestNeighbors(n_neighbors= n_neighbors)\n",
    "    knr.fit(D)\n",
    "    #neighbour graph where the edges are weighted with the euclidean distance\n",
    "    kng = nb.kneighbors_graph(knr,n_neighbors,mode='distance')\n",
    "    #graph distances \n",
    "    Dist_g = ug.graph_shortest_path(kng,directed=False,method='auto')\n",
    "    #the rest is just like mds or PCA (if we want)\n",
    "    nelem = D.shape[0]\n",
    "    J = np.eye(nelem) - (1.0/nelem) * np.ones(nelem)\n",
    "    # Compute matrix B\n",
    "    B = -1.0/2 * np.dot(J,np.dot(pow(Dist_g,2),J))\n",
    "    # SVD decomposition of B  \n",
    "    U,L,V = linalg.svd(B)\n",
    "    return np.dot(U[:,:k],np.sqrt(diag(L)[:k,:k]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "73d5d8a7-0751-da86-d013-7a3df07f6de6",
    "_uuid": "1e7a6d149e65348c743e316af60b36b469bab937"
   },
   "source": [
    "### Own LDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "10852add-9fff-381e-07ae-aeb352a172b0",
    "_uuid": "ba7eadfbb7c52acf0da1df7440dc8e47e53dd908"
   },
   "source": [
    "The previous techniques focus on properties of the feature space\n",
    "- They don‚Äôt take into account any labels/classes the data may have\n",
    "- What if we tried to take into account properties of the data per class?\n",
    "\n",
    "Linear Discriminant Analysis (LDA):\n",
    "- maximize the distance of the class centers\n",
    "- minimize the within-class variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "a0611376-0dd0-eda5-f170-e5e1269142df",
    "_uuid": "c348d8f1a09b3b12bbfa4dc6b30f12c51ac919ad",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def LDA(X, Y):\n",
    "    classLabels = np.unique(Y)\n",
    "    classNum = len(classLabels)\n",
    "    datanum, dim = X.shape\n",
    "    totalMean = np.mean(X,0)\n",
    "\n",
    "\t# partition class labels per label - list of arrays per label\n",
    "\n",
    "    partition = [np.where(Y==label) for label in classLabels]\n",
    "\n",
    "\t# find mean value per class (per attribute) - list of arrays per label\n",
    "    classMean = [(np.mean(X[idx],0),len(idx)) for idx in partition]\n",
    "\n",
    "\t# Compute the within-class scatter matrix\n",
    "    Sw = np.zeros((dim,dim))\n",
    "\t# covariance matrix of each class * fraction of instances for that class \n",
    "    for idx in partition:\n",
    "        Sw=Sw + np.cov(X[idx],rowvar = 0) * len(idx)\n",
    "\t# Compute the between-class scatter matrix\n",
    "    Sb = np.zeros((dim,dim))\n",
    "    for class_mean,class_size in classMean:\n",
    "        temp=(class_mean-totalMean)[:,np.newaxis]\n",
    "        Sb=Sb+class_size*np.dot(temp,np.transpose(temp))\n",
    "\n",
    "\n",
    "\t# Solve the eigenvalue problem for discriminant directions to maximize class separability while simultaneously minimizing\n",
    "\t# the variance within each class\n",
    "    T=np.dot(linalg.inv(Sw),Sb)\n",
    "    eigval, eigvec = linalg.eig(T) \n",
    "\n",
    "\n",
    "    idx = eigval.argsort()[::-1] # Sort eigenvalues\n",
    "    eigvec = eigvec[:,idx] # Sort eigenvectors according to eigenvalues\n",
    "    W = np.real(eigvec[:,:classNum-1]) # eigenvectors correspond to k-1 largest eigenvalues\n",
    "\n",
    "\n",
    "\t# Project data onto the new LDA space\n",
    "    X_lda = np.dot(X,W)\n",
    "\n",
    "\t# project the mean vectors of each class onto the LDA space\n",
    "    projected_centroid = [np.dot(m,W) for m,class_size in classMean]\n",
    "\n",
    "    return W, projected_centroid, X_lda"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "696a8eea-4732-2955-dd09-5354229ae089",
    "_uuid": "dac3f891c1598f03bef20e2c80358c35dd8215e2"
   },
   "source": [
    "### Compare"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "2bfac971-8ba8-7b89-04b2-3c313e165a1a",
    "_uuid": "643c5d63df7b567ba2144c15c869ba4f923cff6a"
   },
   "source": [
    "We are going to compare PCA, MDS, ISomap from Sklearn and from our own implementations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "5009b5ab-b403-67d4-f3c0-2babffda0a3a",
    "_uuid": "9377cc4198b5f3b04c8f827769f65dbefd2f234b",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# The next line is required for plotting only\n",
    "Axes3D\n",
    "\n",
    "iris = pd.read_csv('../input/Iris.csv')\n",
    "X = np.array(iris[[c for c in iris.columns if c != \"Species\" and c!='Id']])\n",
    "Y_iris = iris[\"Species\"]\n",
    "color = LabelEncoder().fit_transform(Y_iris)\n",
    "\n",
    "\n",
    "n_components=2\n",
    "n_neighbors=5\n",
    "\n",
    "fig = plt.figure(figsize=(15, 8))\n",
    "ax = fig.add_subplot(251, projection='3d')\n",
    "ax.scatter(X[:, 0], X[:, 1], X[:, 2], c=color, cmap=plt.cm.Spectral)\n",
    "ax.view_init(4, -72)\n",
    "\n",
    "#------PCA--------our implementation\n",
    "t0 = time()\n",
    "(Y,perc)=pca1(X,n_components)\n",
    "t1 = time()\n",
    "print(\"PCA(imp): %.2g sec\" % (t1 - t0))\n",
    "ax = fig.add_subplot(252)\n",
    "plt.scatter(Y[:, 0], Y[:, 1], c=color, cmap=plt.cm.Spectral)\n",
    "plt.title(\"PCA(imp) (%.2g sec)\" % (t1 - t0))\n",
    "ax.xaxis.set_major_formatter(NullFormatter())\n",
    "ax.yaxis.set_major_formatter(NullFormatter())\n",
    "plt.axis('tight')\n",
    "#-----------------\n",
    "\n",
    "#------MDS--------our implementation (classical MDS)\n",
    "t0 = time()\n",
    "D=spd.squareform(spd.pdist(X,'euclidean'))\n",
    "Y=mds(D,n_components)\n",
    "t1 = time()\n",
    "print(\"MDS(imp): %.2g sec\" % (t1 - t0))\n",
    "ax = fig.add_subplot(253)\n",
    "plt.scatter(Y[:, 0], Y[:, 1], c=color, cmap=plt.cm.Spectral)\n",
    "plt.title(\"MDS(imp) (%.2g sec)\" % (t1 - t0))\n",
    "ax.xaxis.set_major_formatter(NullFormatter())\n",
    "ax.yaxis.set_major_formatter(NullFormatter())\n",
    "plt.axis('tight')\n",
    "#-----------------\n",
    "#------isomap--------our implementation (with MDS)\n",
    "t0 = time()\n",
    "Y=isomap(X,n_components,n_neighbors)\n",
    "t1 = time()\n",
    "print(\"Isomap(imp): %.2g sec\" % (t1 - t0))\n",
    "ax = fig.add_subplot(254)\n",
    "plt.scatter(Y[:, 0], Y[:, 1], c=color, cmap=plt.cm.Spectral)\n",
    "plt.title(\"Isomap(imp) (%.2g sec)\" % (t1 - t0))\n",
    "ax.xaxis.set_major_formatter(NullFormatter())\n",
    "ax.yaxis.set_major_formatter(NullFormatter())\n",
    "plt.axis('tight')\n",
    "#-----------------\n",
    "\n",
    "#----PCA---------- sklearn implementation \n",
    "\n",
    "t0 = time()\n",
    "pca = PCA(n_components=n_components)\n",
    "Y = pca.fit_transform(X)\n",
    "t1 = time()\n",
    "print(\"PCA: %.2g sec\" % (t1 - t0))\n",
    "ax = fig.add_subplot(257)\n",
    "plt.scatter(Y[:, 0], Y[:, 1], c=color, cmap=plt.cm.Spectral)\n",
    "plt.title(\"PCA (%.2g sec)\" % (t1 - t0))\n",
    "ax.xaxis.set_major_formatter(NullFormatter())\n",
    "ax.yaxis.set_major_formatter(NullFormatter())\n",
    "plt.axis('tight')\n",
    "#--------------------\n",
    "#----MDS---------- sklearn implementation (Stress minimization-majorization algorithm SMACOF)\n",
    "t0 = time()\n",
    "mds = manifold.MDS(n_components, max_iter=100, n_init=1)\n",
    "Y = mds.fit_transform(X)\n",
    "t1 = time()\n",
    "print(\"MDS: %.2g sec\" % (t1 - t0))\n",
    "ax = fig.add_subplot(258)\n",
    "plt.scatter(Y[:, 0], Y[:, 1], c=color, cmap=plt.cm.Spectral)\n",
    "plt.title(\"MDS (%.2g sec)\" % (t1 - t0))\n",
    "ax.xaxis.set_major_formatter(NullFormatter())\n",
    "ax.yaxis.set_major_formatter(NullFormatter())\n",
    "plt.axis('tight')\n",
    "#---------------\n",
    "#----Isomap---------- sklearn implementation (with kernel PCA)\n",
    "t0 = time()\n",
    "Y = manifold.Isomap(n_neighbors, n_components).fit_transform(X)\n",
    "t1 = time()\n",
    "print(\"Isomap: %.2g sec\" % (t1 - t0))\n",
    "ax = fig.add_subplot(259)\n",
    "plt.scatter(Y[:, 0], Y[:, 1], c=color, cmap=plt.cm.Spectral)\n",
    "plt.title(\"Isomap (%.2g sec)\" % (t1 - t0))\n",
    "ax.xaxis.set_major_formatter(NullFormatter())\n",
    "ax.yaxis.set_major_formatter(NullFormatter())\n",
    "plt.axis('tight')\n",
    "#--------------------\n",
    "\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "bb8781ee-8264-8328-1358-9c98d2b6c078",
    "_uuid": "ec2bac9510ea5d69cbce7e2a365080d17d953c95"
   },
   "source": [
    "### LDA vs PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "a4296998-cb6b-026b-11bf-bd0865e48faf",
    "_uuid": "9456ee6935b67d5b7a8f2fea380e3010e5aceddb",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "c=['r','g','b','k']\n",
    "colors=[c[int(i-1)] for i in color]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "ece1f7eb-8d3f-7850-557f-69c9bc9290eb",
    "_uuid": "e81f11015508217edf499688dd9b608f29178646",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# get LDA projection\n",
    "W, projected_centroids, X_lda = LDA(X, color)\n",
    "\n",
    "#perform PCA to compare with LDA\n",
    "pca = PCA(n_components=2)\n",
    "Y = pca.fit_transform(X)\n",
    "\n",
    "#PLOT them side by side\n",
    "fig = plt.figure(figsize=(15, 8))\n",
    "ax = fig.add_subplot(121)\n",
    "ax.scatter(X_lda[:,0], X_lda[:,1],color=colors)\n",
    "for ar in projected_centroids:\n",
    "\tax.scatter(ar[0], ar[1],color='k',s=100)\n",
    "ax = fig.add_subplot(122)\n",
    "ax.scatter(Y[:,0], Y[:,1],color=colors)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "df76e53c-08d1-9269-a4db-70763c049fd1",
    "_uuid": "1173e07190e2ab1fd14dc960a4b27c1ce3069ad2",
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "61644604-8291-adac-550f-f4706ba74829",
    "_uuid": "e5cbef144533fae24030b00c35842dd02eefc0b6",
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "_change_revision": 0,
  "_is_fork": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
